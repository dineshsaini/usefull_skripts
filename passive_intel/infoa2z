#!/usr/bin/bash

#v_browser="0";		# which browser to use (0 => wget, 1=> curl)
b_opts="";		# use -= [pass options] -= to pass options to browser, this will give higer control
			# of browser to user, will be good at later.

re_type="2";		# type of regex; fixed(1), basic(2)[default], extended(3), PCRE(4)
re_mail="";		# regex to be used to find emails.
re_domain="";		# regex to be used to find domains.
re_mob="";		# regex to be used to find contacts.
re_raw="";		# custom regex to be used to find custom info.

t_tar="0";		# target to use ( 1=> domain, 2=> people, 3 => raw)
t_domain="";		# target domain
t_people="";		# target people

s_google="0";		# search google (0 => disable, 1 => enable)
g_guess="0";		# along with the keywords/query provided, guess the results also. (0 => don't guess, 1 => guess)
g_nop="4"; 		# google no of pages (-1 as inf)
g_rpp="30"; 		# results per page (10, 20, 30, 50, 100)
g_tr="-1";		# total results (-1 for as many, +ve => stop after this much)
g_query="";		# user's provided query
g_pre_q="0";		# query pre formated (0 => need to format, 1 => do not process `$g_query` value, use as-is)

e_domain="1";		# extract domain (0 => flase, 1 => true), use re_doamin to extract
e_email="0";		# extract email addresses (0 => false, 1 => true), use re_mail to extract
e_mob="0";		# extract contact numbers (0 => false, 1 => true), use re_mob to extract ## only if target is people
e_raw="0";		# extract raw info (0 => false, 1=> true), use re_raw to extract ## only if re_raw is defined	
e_url="1";		# extract urls (0 => false, 1=> true), use re_url to extract  ## only if target is people

h_subd="0";		# harvest subdomain in seperate dictionary file
h_snaps="0";		# harvest snapshot of urls ## only if e_url is 1

r_freq="0";		# report frequecy of occurence of domain

o_file="";		# output file to use
o_subd_file="";		# save subdomains in this, use - to provide default file

v_delay="1";		# min. delay between requests values (0,\[0-9]+[sm]\)
v_verbose=0;		# be verbose
v_count=0;		# current results count
v_verbose_f="";		# verbose file name

FLAG_BGPID="";

shopt -s extglob;

function init {
	e_verbose;

	trap cleanup SIGINT SIGQUIT SIGKILL SIGSTOP SIGTERM SIGHUP SIGTSTP EXIT;
}

function cleanup {
	d_verbose;
}

function e_verbose {
	if [[ $v_verbose -ge 0 ]]; then
		v_verbose_f=$(tempfile);
		tail -f $v_verbose_f &
		FLAG_BGPID="$!";
	fi;
}

function d_verbose {
	if [[ x"$FLAG_BGPID" != "x" ]]; then
		kill $FLAG_BGPID > /dev/null;
		FLAG_BGPID="";
		rm -f $v_verbose_f > /dev/null;
	fi;
}

function help {
	echo -e "-h, --help\t\t\t\tPrint help msg.";
	echo -e "-w, --wait, --delay <time(m|s)>\t\tMinimum time to wait between requests. Specify as (0, ns, nm), where n is +ve number";
	echo -e "-t, --target [options]\t\t\tSpecify target.";
		echo -e "\t-d, --domain <domain>\t\tSpecify target domain.";
		echo -e "\t-p, --people <people>\t\tSpecify people to search";
		echo -e "\t-r, --raw\t\t\tSearch raw info, without target";
	echo -e "-g, --google [options]\t\t\tGoogle Search related options";
		echo -e "\t--dork <google dork>\t\tSpecify dork to use";
		echo -e "\t-s, --search <query>\t\tSearch query";
		echo -e "\t-G, --guess\t\t\tGuess the dork";
		echo -e "\t-p, --pages <number>\t\tNo. of pages to crawl";
		echo -e "\t-n, --psize <number>\t\tNo. of results to fetch per page";
		echo -e "\t-t, --stop <number>\t\tStop after this much results";
	echo -e "-r, --re [options]\t\t\tSpecify regex";
		echo -e "\t-m, --mail <regex>\t\tRegex to search for emails";
		echo -e "\t-d, --domain <regex>\t\tRegex for how to search for domains";
		echo -e "\t-c, --mob <regex>\t\tRegex for how to search for contact numbers";
		echo -e "\t--raw <regex>\t\t\tRegex to use when using raw target";
		echo -e "\t-E, --extended-regexp\t\tType of search to use";
		echo -e "\t-F, --fixed-strings\t\tType of search to use";
		echo -e "\t-G, --basic-regexp\t\tType of search to use (default)";
		echo -e "\t-P, --perl-regexp\t\tType of search to use";
	echo -e "-= [options] -=\t\t\t\tBrowser options to pass to wget";
	echo -e "-H, --harvest [options]\t\t\tSpecift What info to harvest";
		echo -e "\t-d, --domain\t\t\tHarvest domains";
		echo -e "\t-m, --mail\t\t\tHarvest mails";
		echo -e "\t-c, --mob\t\t\tHarvest numbers";
		echo -e "\t-u, --url\t\t\tHarvest urls";
		echo -e "\t-r, --raw\t\t\tHarvest raw info, only works if regex is specified";
		echo -e "\t-S, --snaps\t\t\tTry to get google cache of urls";
		echo -e "\t-s, --subdomain <file>\t\tcollect subdomains to build dictionary, provide '-' (hyphen) if using default file creation";
		echo -e "\t-a, --all\t\t\tHarvest everything";
	echo -e "-o, --out <file>\t\t\tSpecify output file";
	echo -e "-v, --verbose\t\t\t\tBe verbose, specify this multiple times like (-v -v) for more verbosity";
	exit 0;
}

function queryEnc {
	[[ $v_verbose -ge 2 ]] && (echo "encrypting:: ($1)" >> $v_verbose_f);
	echo $(echo "$1" | sed -e 's/%/%25/g' -e 's/!/%21/g' \
		-e 's/"/%22/g' -e 's/#/%23/g' -e 's/\$/%24/g' \
	       	-e 's/\&/%26/g' -e 's/'\''/%27/g' -e 's/(/%28/g' \
		-e 's/)/%29/g' -e 's/+/%2B/g' -e 's/,/%2C/g' \
		-e 's/\//%2F/g' -e 's/:/%3A/g' -e 's/;/%3B/g' \
		-e 's//%3E/g' -e 's/?/%3F/g' -e 's/@/%40/g' \
		-e 's/\[/%5B/g' -e 's/\\/%5C/g' -e 's/\]/%5D/g' \
		-e 's/\^/%5E/g' -e 's/`/%60/g' -e 's/{/%7B/g' \
		-e 's/|/%7C/g' -e 's/}/%7D/g' -e 's/~/%7E/g' \
		-e 's/ /+/g' -e 's/=/%3D/g' -e 's/</%3C/g'  \
		-e 's/>/%3E/g');
}

# $1 => query (required) 
# $2 => page navigation query string (required)
# $3 => file 
# returns response file location
function google_srch {
	local query="$1";
	if [[ x"$query" == "x" ]]; then
		echo "-1";
		return;
	fi;
	[[ $v_verbose -ge 2 ]] && (echo "recieved querystring ($query)" >> $v_verbose_f);
	
	local nqstr="$2";
	if [[ x"$nqstr" == "x" ]]; then
		echo "-2";
		return;
	fi;
	[[ $v_verbose -ge 2 ]] && (echo "recieved query params ($nqstr)" >> $v_verbose_f);

	local gws="https://www.google.com";
	local host="www.google.com";
	if [[ x"$3" == "x1" ]]; then
		gws="http://webcache.googleusercontent.com";
		host="webcache.googleusercontent.com";
	fi;
	
	[[ $v_verbose -ge 2 ]] && (echo "using gws: $gws" >> $v_verbose_f);
	[[ $v_verbose -ge 2 ]] && (echo "using host: $host" >> $v_verbose_f);

	local url="$gws/search?gbv=1&sei=rlcLXr_JAdXE4-EP6oOmcA&q=$query&$nqstr";
	[[ $v_verbose -ge 1 ]] && (echo "using url: $url" >> $v_verbose_f);
	local oFile="$(tempfile)";
	local tFile="$(tempfile)";

	[[ $v_verbose -ge 2 ]] && (echo "found browser options:: $b_opts" >> $v_verbose_f);
	wget "$url" --header="'Host: $host'" \
		--header='User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:68.0) Gecko/20100101 Firefox/68.0' \
		--header='Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8' \
		--header='Accept-Language: en-US,en;q=0.5' \
		--header='Accept-Encoding: gzip, deflate' \
		--header='Referer: https://www.google.com/' \
		--header='DNT: 1' \
		--header='Connection: keep-alive' \
		--header='Upgrade-Insecure-Requests: 1'  \
		--header='Cache-Control: max-age=0, no-cache' \
		--header='Pragma: no-cache'  \
		--header='TE: Trailers' \
		-O $tFile \
		--quiet $b_opts ;
	local retval=$?;

	if [[ $retval -ne 0 ]]; then
		[[ $v_verbose -ge 2 ]] && (echo "wget returned error: ($retval)" >> $v_verbose_f);
		echo $retval;
		return;
	fi;

	if [[ x"$tFile" != "x" ]]; then
		[[ $v_verbose -ge 2 ]] && (echo "got file: $oFile" >> $v_verbose_f);
		cat $tFile | gunzip > $oFile;
		rm -f $tFile;
	fi;
	echo $oFile;
}

function snapshot {
	[[ $v_verbose -ge 1 ]] && (echo "trying snapshot for $1" >> $v_verbose_f);
	sfile=$(google_srch "$(queryEnc "cache:$1")" "oq=&aqs=" 1);
	echo $sfile;
}

function getList {
	local htmlPage="$1";
	local outPage="$2";
	
	[[ $v_verbose -ge 1 ]] && (echo "Fetching info from $1" >> $v_verbose_f);

	local retval="$( (/usr/bin/env perl -)< <(cat <<__END__
	use v5.28.1;
	use strict;
	use warnings;
	use Mojo::DOM;
	use Mojo::File;
	
	my \$htmlPage = "$htmlPage";
	my \$outFile = "$outPage";
	my \$hasNext = 1;
	my \$rcount = 0;
	
	sub processHTML {	
		my \$file = Mojo::File->new(\$htmlPage);
		my \$doc = Mojo::DOM->new(\$file->slurp);
		my \$fd;
		open(\$fd, '>', \$outFile) or return "0,0";
	
		for my \$i (\$doc->find('#main>div')->each) {
			my \$j = Mojo::DOM->new(\$i);
			for my \$k (\$j->find('a[href^="/url?q="]')->each) {
				\$rcount++;
				my \$link = \$k->{href};
				\$link =~ s/&.*\$//;
				\$link =~ s/^\/url\?q=//;
				my \$title = \$k->children->first->all_text;
				my \$summary = \$k->parent->next->next->content;
				\$summary =~ s/\n/ /g;
				# say "link >> \$link";
				# say "title >> \$title";
				# say "summary >> \$summary";		
				print \$fd "\$title\n\$link\n\$summary\n";
			}
		}
		close \$fd;
	
		my \$foot = "";
		for my \$i (\$doc->find('#main footer')->each){
			\$foot.=\$i->all_text;
		}
		\$hasNext = 0 if \$foot !~ /Next >/ ;
	
		return "\$hasNext,\$rcount";
	}
	my \$r=processHTML;
	say \$r;
__END__
	) )";
	echo $retval;
}

function analyse {
	local page=$1;
	local hasNext=1;
	local gotAll=0;
	local cFile=$2;

	local tResults=$(tempfile);	

	local r=$(getList $page $tResults);

	hasNext=$(echo $r | cut -d, -f1);
	local rcount=$(echo $r | cut -d, -f2);

	if [[  $g_tr -ne -1 ]] && [[ $(($v_count + $rcount)) -ge $g_tr ]]; then
		rcount=$(($g_tr-$v_count));
		gotAll=1;
	fi;

	local i=0;
	local c=0;
	while IFS= read -r line; do
		if [[ $c -le $rcount ]]; then
			case $(( $((++i)) % 3 )) in 
				1)
					echo "$line" >> $cFile;
					;;
				2)
					echo "$line" >> $cFile;
					;;
				0)
					(( ++c ));
					echo "$line" >> $cFile;
					;;
			esac;
		else
			break;
		fi;
	done < $tResults;
	echo "$hasNext,$gotAll";
}

function google {
	local query="$1";
	local pageN=0;
	local hasNext=1;
	local gotAll=0;
	local lFile="$( ([[ x"$2" == "x" ]] && tempfile) || echo $2)";

	[[ $v_verbose -ge 1 ]] && (echo "google query for ($query)" >> $v_verbose_f);
	case $(($g_rpp/10)) in
		1)
			g_rpp=10;
			;;
		2)
			g_rpp=20;
			;;
		3)
			g_rpp=30;
			;;
		4)
			g_rpp=40;
			;;
		5)
			g_rpp=50;
			;;
		10)
			g_rpp=100;
			;;
		*)
			g_rpp=30;
			;;
	esac;
	[[ $v_verbose -ge 2 ]] && (echo "using results per page: $g_rpp" >> $v_verbose_f);
	if [[ $g_nop -le 0 ]]; then
		$g_nop=4;
	fi;
	[[ $v_verbose -ge 2 ]] && (echo "using number of page: $g_nop" >> $v_verbose_f);
	
	while [[ $pageN -lt $g_nop ]] && [[ $hasNext -ne 0 ]]  && [[ $gotAll -ne 1 ]]; do
		local pageQuery="start=$((pageN++))&num=$g_rpp";
		[[ $v_verbose -ge 2 ]] && (echo "current query: $pageQuery" >> $v_verbose_f);

		local oFile="$(google_srch $query $pageQuery)";
		[[ $(echo $oFile | grep "^/tmp/" | wc -l) -eq 0 ]] && break;  # got error, exit
		local c="$(analyse $oFile $lFile)";  # returns tuple (hasNext,gotAll) values after analyse 
		
		[[ -z "$c" ]] && break;
		hasNext=$(echo $c | cut -d, -f1);
		gotAll=$(echo $c | cut -d, -f2);
		
		[[ $v_verbose -ge 2 ]] && (echo "do we have more pages: $hasNext" >> $v_verbose_f);
		[[ $v_verbose -ge 2 ]] && (echo "got all results: $gotAll" >> $v_verbose_f);
		[[ $v_verbose -ge 2 ]] && (echo "sleeping for $v_delay sec" >> $v_verbose_f);
		sleep $v_delay;
	done;
	echo $lFile;
}

function search {
	local resFile="$(tempfile)";

	if [[ $s_google -eq 1 ]]; then
		[[ $v_verbose -ge 1 ]] && (echo "trying google search" >> $v_verbose_f);
		# without guess
		if [[ $g_guess -eq 0 ]]; then
			if [[ $g_pre_q -eq 1 ]]; then
				[[ $v_verbose -ge 2 ]] && (echo "using dork: $g_query" >> $v_verbose_f);
				resFile=$(google "$(queryEnc "$g_query")" $resFile);
			else
				case $t_tar in
					1)
						resFile=$(google "$(queryEnc "$g_query")+site%3A$(queryEnc "$t_domain")" $resFile);
						;;
					2|3)
						resFile=$(google "$(queryEnc "$g_query")" $resFile);
						;;
				esac;
			fi;
		fi;
		# guess also
		[[ $g_guess -eq 1 ]] && [[ $v_verbose -ge 2 ]] && (echo "doing guess" >> $v_verbose_f);
		[[ $g_guess -eq 1 ]] && case $t_tar in
			1)
				resFile=$(google "site%3A$(queryEnc "$t_domain")+-www*" $resFile);
				;;
			2)
				resFile=$(google "$(queryEnc "$t_people")+site%3Afacebook.com" $resFile);
				resFile=$(google "$(queryEnc "$t_people")+site%3Alinkedin.com" $resFile);
				resFile=$(google "$(queryEnc "$t_people")+site%3Atwitter.com" $resFile);
				resFile=$(google "$(queryEnc "$t_people")+site%3Ainstagram.com" $resFile);
				;;
		esac;
	fi;
	echo $resFile;
}

function filter {
	local l="$1";
	local c=$2;
	local r="";
	case $c in 
		1)
			[[ $v_verbose -ge 1 ]] && (echo "extracting people name on basis of ($t_people)" >> $v_verbose_f);
			[[ $v_verbose -ge 2 ]] && (echo "filter for people, trying lazy" >> $v_verbose_f);
			local p="$(echo "$t_people" | sed -e 's/[[:space:]]\+/ /g' -e 's/ /.*?/g')";
			r="$(echo "$l" | grep -P -o "$p")";
			
			if [[ $(echo "$r" | wc -l) -eq 0 ]]; then
				[[ $v_verbose -ge 2 ]] && (echo "filtering for people, trying greedy" >> $v_verbose_f);
				p="$(echo "$t_people" | sed -e 's/[[:space:]]\+/ /g' -e 's/ /.*/g')";
				r="$(echo "$l" | grep -P -o "$p")";
			fi;
			;;
		2)
			[[ $v_verbose -ge 1 ]] && (echo "extracting domain" >> $v_verbose_f);
			if [[ x"$re_domain" != "x" ]]; then
				[[ $v_verbose -ge 2 ]] && (echo "using regex: $re_domain" >> $v_verbose_f);
				case $re_type in 
					1)
						r="$(echo "$l" | grep -F -o "$re_domain")";
						;;
					2)
						r="$(echo "$l" | grep -G -o "$re_domain")";
						;;
					3)
						r="$(echo "$l" | grep -E -o "$re_domain")";
						;;
					4)
						r="$(echo "$l" | grep -P -o "$re_domain")";
						;;
				esac;
			else
				r="$(echo "$l" | sed -e 's/^\(\(ht\|f\)tp\(s\)\?\)\?\(:\)\?\(\/\)\?\(\/\)\?//' \
				-e 's/\/.*//')";
			fi;
			;;
		3)
			[[ $v_verbose -ge 1 ]] && (echo "extracting subdomain" >> $v_verbose_f);
			r="$(echo "$l" | sed -e 's/^\(\(ht\|f\)tp\(s\)\?\)\?\(:\)\?\(\/\)\?\(\/\)\?//' \
				-e 's/\/.*//' \
				-e 's/^\(www\.\)//' \
				-e 's/\.\?[^.]*\.[^.]*$//')";
			;;
		4)
			[[ $v_verbose -ge 1 ]] && (echo "extracting contacts" >> $v_verbose_f);
			if [[ x"$re_mob" != "x" ]]; then
				[[ $v_verbose -ge 2 ]] && (echo "using regex: $re_mob" >> $v_verbose_f);
				case $re_type in 
					1)
						r="$(echo "$l" | grep -F -o "$re_mob")";
						;;
					2)
						r="$(echo "$l" | grep -G -o "$re_mob")";
						;;
					3)
						r="$(echo "$l" | grep -E -o "$re_mob")";
						;;
					4)
						r="$(echo "$l" | grep -P -o "$re_mob")";
						;;
				esac;
			else
				r="$(echo "$l" | grep -o '\(([0-9]\{3\})\|[0-9]\{3\}\)[ -]\?[0-9]\{3\}[ -]\?[0-9]\{4\}')";
			fi;
			;;
		5)	
			[[ $v_verbose -ge 1 ]] && (echo "extracting mails" >> $v_verbose_f);
			if [[ x"$re_mail" != "x" ]]; then
				[[ $v_verbose -ge 2 ]] && (echo "using regex: $re_mail" >> $v_verbose_f);
				case $re_type in 
					1)
						r="$(echo "$l" | grep -F -o "$re_mail")";
						;;
					2)
						r="$(echo "$l" | grep -G -o "$re_mail")";
						;;
					3)
						r="$(echo "$l" | grep -E -o "$re_mail")";
						;;
					4)
						r="$(echo "$l" | grep -P -o "$re_mail")";
						;;
				esac;

			else
				r="$(echo "$l" | grep -E -o "\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,6}\b")";
			fi;
			;;
		6)
			[[ $v_verbose -ge 1 ]] && (echo "extracting raw info" >> $v_verbose_f);
			if [[ x"$re_raw" == "x" ]]; then
				[[ $v_verbose -ge 2 ]] && (echo "no regex found" >> $v_verbose_f);
				r="error: regex is not defined";
			else
				[[ $v_verbose -ge 2 ]] && (echo "using regex: $re_raw" >> $v_verbose_f);
				case $re_type in 
					1)
						r="$(echo "$l" | grep -F -o "$re_raw")";
						;;
					2)
						r="$(echo "$l" | grep -G -o "$re_raw")";
						;;
					3)
						r="$(echo "$l" | grep -E -o "$re_raw")";
						;;
					4)
						r="$(echo "$l" | grep -P -o "$re_raw")";
						;;
				esac;
			fi;
			;;
	esac;
	echo "$r";
}

function harvest {
	local rFile=$1;
	local i=0;
	local snapdir="$(dirname $o_file)/snaps";

	if [[ $h_snaps -eq 1 ]]; then
		[[ $v_verbose -ge 1 ]] && (echo "using snapshot dir: ($snapdir)" >> $v_verbose_f);
		mkdir -p $snapdir; 
	fi;
	[[ $v_verbose -ge 1 ]] && (echo "writing records to file: $o_file" >> $v_verbose_f);
	echo '<?xml version="1.0"?>' >$o_file;
	echo '<records>' >>$o_file;
	while IFS= read -r line; do
		[[ $(( $i % 3 )) -eq 0 ]] && (echo '<record>' >>$o_file);
		case $(( $((++i)) % 3 )) in 
			1)
				if [[ $t_tar -eq 2 ]]; then
					echo "<title>$(filter "$line" 1 | tr '\n' '|' | sed 's/|$//')</title>" >> $o_file;
				fi;
				if [[ $t_tar -eq 3 ]] && [[ $e_raw -eq 1 ]]; then
					echo "<raw1>$(filter "$line" 6 | tr '\n' '|' | sed 's/|$//')</raw1>" >> $o_file;
				fi;
				;;
			2)
				if [[ $e_url -eq 1 ]]; then
					echo "<url>$line</url>" >> $o_file;
				fi;

				if [[ $t_tar -ne 2 ]] && [[ $e_domain -eq 1 ]]; then
					echo "<host>$(filter "$line" 2 | tr '\n' '|' | sed 's/|$//')</host>" >> $o_file;
				fi;

				if [[ $h_subd -eq 1 ]]; then
					if [[ x"$o_subd_file" == "x-" ]] || [[ x"$o_subd_file" == "" ]]; then
						echo "$(filter "$line" 3)" >> $o_file.subdomain.dic;
					else
						echo "$(filter "$line" 3)" >> $o_subd_file;
					fi;
				fi;

				if [[ $t_tar -eq 3 ]] && [[ $e_raw -eq 1 ]]; then
					echo "<raw2>$(filter "$line" 6 | tr '\n' '|' | sed 's/|$//')</raw2>" >> $o_file;
				fi;

				if [[ $h_snaps -eq 1 ]]; then
					local sfile=$(snapshot $line);
					if [[ $(echo "$sfile" |grep "^/tmp/" | wc -l) -eq 1 ]]; then
						mv $sfile $snapdir;
						fname=$(basename $sfile);
						echo "<snapshot>$snapdir/$fname</snapshot>" >> $o_file;
					fi;
				fi;
				;;
			0)
				if [[ $e_email -eq 1 ]]; then
					echo "<email>$(filter "$line" 5 | tr '\n' '|' | sed 's/|$//')</email>" >> $o_file;
				fi;
				
				if [[ $e_mob -eq 1 ]]; then
					echo "<contact>$(filter "$line" 4 | tr '\n' '|' | sed 's/|$//')</contact>" >> $o_file;
				fi;

				if [[ $t_tar -eq 3 ]] && [[ $e_raw -eq 1 ]]; then
					echo "<raw3>$(filter "$line" 6 | tr '\n' '|' | sed 's/|$//')</raw3>" >> $o_file;
				fi;
				;;
		esac;
		[[ $(( $i % 3 )) -eq 0 ]] && (echo '</record>' >>$o_file);
	done < $rFile;
	echo '</records>' >>$o_file;
	[[ $v_verbose -ge 1 ]] && (echo "finish writing records" >> $v_verbose_f);
}

while [[ $# -gt 0 ]]; do
	case $1 in 
		-h|--help)
			help;
			;;
		-w|--delay|--wait)
			shift;
			case $1 in 
				0)
					v_delay=0;
					shift;
					;;
				+([0-9])s)
					v_delay="${1%%s}";
					shift;
					;;
				+([0-9])m)
					v_delay="$((${1%%m}*60))";
					;;
				*)
					echo "ERROR: invalid '$1' delay value.";
					exit 1;
					;;
			esac;
			;;
		-t|--target)
			shift;
			case $1 in
				-d|--domain)
					t_tar=1;
					t_domain="$2";
					shift;
					shift;
					;;
				-p|--people)
					# TODO: integrate more options here later
					t_tar=2;
					t_people="$2";
					shift;
					shift;
					;;
				-r|--raw)
					t_tar=3;
					shift;
					;;
			esac;
			;;
		-g|--google)
			shift;
			flag=0;
			s_google=1;
			while [[ $flag -eq 0 ]]; do
				case $1 in
					--dork)  			# do not guess use this exact query to search target
						g_query="$2";
						g_pre_q=1;
						shift;
						shift;
						;;
					-s|--search)			# do not guess use these keywords to search target
						g_query="$2";
						g_pre_q=0;
						shift;
						shift;
						;;
					-G|--guess)			# combine guess result also with (dork|search) results
						g_guess="1";
						shift;
						;;
					-p|--pages)			
						g_nop=$2;
						shift;
						shift;
						;;
					-n|--psize)
						g_rpp=$2;
						shift;
						shift;
						;;
					-t|--stop)
						g_tr=$2;
						shift;
						shift;
						;;
					*)
						flag=1;
						;;
				esac;
			done;
			;;
		-r|--re)
			shift;
			flag=0;
			while [[ $flag -eq 0 ]]; do
				case $1 in 
					-m|--mail)
						re_mail="$2";
						shift;
						shift;
						;;
					-d|--domain)
						re_domain="$2";
						shift;
						shift;
						;;
					-c|--mob)
						re_mod="$2";
						shift;
						shift;
						;;
					--raw)
						re_raw="$2";
						shift;
						shift;
						;;
					-E|--extended-regexp)
						re_type=3;
						shift;
						;;
					-F|--fixed-strings)
						re_type=1;
						shift;
						;;
					-G|--basic-regexp)
						re_type=2;
						shift;
						;;
					-P|--perl-regexp)
						re_type=4;
						shift;
						;;
					*)
						flag=1;
						;;
				esac;
			done;
			;;
		-=)
			shift;
			while [[ "$1" != "-=" ]]; do
				b_opts="$b_opts $1";
				shift;
			done;
			shift;
			;;
		-H|--harvest)
			shift;
			flag=0;	
			while [[ $flag -eq 0 ]]; do
				case $1 in
					-d|--domain)
						e_domain=1;
						shift;
						;;
					-m|--mail)
						e_email=1;
						shift;
						;;
					-c|--mob)
						e_mob=1;
						shift;
						;;
					-u|--url)
						e_url=1;
						shift;
						;;
					-r|--raw)
						e_raw=1;
						shift;
						;;
					-S|--snaps)
						h_snaps=1; # only for urls and if url harvest is on
						shift;
						;;
					-s|--subdomain)
						h_subd=1;
						o_subd_file="$2";
						shift;
						shift;
						;;
					-a|--all)
						e_domain=1;
						e_email=1;
						e_mob=1;
						e_url=1;
						e_raw=1;
						h_snaps=1;
						h_subd=1;
						o_subd_file="-";
						shift;
						;;
					*)
						flag=1;
						;;
				esac;
			done;
			;;
		-o|--out)
			o_file="$2";
			shift;
			shift;
			;;
		-v|--verbose)
			shift;
			v_verbose=$((++v_verbose));
			;;
		*)
			echo "ERROR: $1 not defined";
			exit 1;
			;;
	esac;
done;

init;
# if no query is provided then guess anyways
if [[ x"$g_query" == "x" ]]; then
	[[ $v_verbose -ge 1 ]] && (echo "query is empty, enabling guess" >> $v_verbose_f);
	g_guess=1;
fi;

if ! [[ $t_tar -gt 0 ]]; then
	echo "ERROR: Target is not defined";
	exit 1;
fi;

harvest "$(search)";
